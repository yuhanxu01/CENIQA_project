# GMMæ”¹è¿›æ–¹æ¡ˆ - åŸºäº2024-2025æœ€æ–°ç ”ç©¶

## é—®é¢˜è¯Šæ–­

å½“å‰GMMå’Œæ— GMMåœ¨éªŒè¯é›†ä¸Šè¡¨ç°ä¸€è‡´ï¼Œä¸»è¦åŸå› ï¼š

### 1. **Posterioråˆ©ç”¨ä¸è¶³**
- Posteriorsåªæ˜¯ç®€å•concatenateï¼Œregressorå¯èƒ½å¿½ç•¥å®ƒä»¬
- ç¼ºå°‘gating/routingæœºåˆ¶æ¥é€‰æ‹©æ€§ä½¿ç”¨ä¸åŒclustersçš„ä¿¡æ¯

### 2. **Clusterç¼ºå°‘è¯­ä¹‰æ€§**
- GMM clusteringæ˜¯å®Œå…¨æ— ç›‘ç£çš„
- æ²¡æœ‰ä¿è¯ä¸åŒclusterså¯¹åº”ä¸åŒdistortion types
- Cluster lossè¿‡äºç®€å•ï¼ˆåªmaximize confidenceï¼‰

### 3. **æ¶æ„è€¦åˆä¸è¶³**
- GMMå’Œregressoræ˜¯sequentialçš„ï¼Œè€Œécollaborative
- ç¼ºå°‘feature-posterioräº¤äº’æœºåˆ¶

---

## æ”¹è¿›æ–¹æ¡ˆï¼ˆä»ç®€å•åˆ°å¤æ‚ï¼‰

### â­ **æ–¹æ¡ˆ1ï¼šMixture of Expert Regressors** (æ¨èé¦–é€‰)

**æ ¸å¿ƒæ€æƒ³**ï¼šæ¯ä¸ªclusterå¯¹åº”ä¸€ä¸ªä¸“é—¨çš„quality regressorï¼Œç”¨posteriorä½œä¸ºgating weights

```python
class MoECENIQA(nn.Module):
    """CENIQA with Mixture of Expert Regressors"""
    def __init__(self, config):
        super().__init__()
        self.backbone = build_backbone(...)
        self.feature_proj = nn.Linear(...)

        # GMM for clustering
        self.gmm = DifferentiableGMM(n_clusters, feature_dim)

        # ğŸ”¥ æ¯ä¸ªclusterä¸€ä¸ªexpert regressor
        self.experts = nn.ModuleList([
            build_regressor(config.regressor_type, feature_dim, hidden_dim)
            for _ in range(n_clusters)
        ])

        # å¯é€‰ï¼šgating networkæ¥refine posteriors
        self.gating = nn.Sequential(
            nn.Linear(feature_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, n_clusters),
            nn.Softmax(dim=-1)
        )

    def forward(self, x):
        features = self.extract_features(x)

        # Get cluster posteriors
        posteriors = self.gmm(features)  # [B, K]

        # å¯é€‰ï¼šç”¨gating network refine
        gates = self.gating(features)  # [B, K]
        weights = posteriors * gates  # element-wise product
        weights = weights / (weights.sum(dim=1, keepdim=True) + 1e-8)

        # æ¯ä¸ªexperté¢„æµ‹quality
        expert_predictions = []
        for expert in self.experts:
            pred = expert(features)  # [B, 1]
            expert_predictions.append(pred)

        expert_predictions = torch.stack(expert_predictions, dim=1)  # [B, K, 1]

        # Weighted combination
        quality_score = torch.sum(weights.unsqueeze(-1) * expert_predictions, dim=1)

        return quality_score.squeeze(-1)
```

**è®­ç»ƒæ”¹è¿›**ï¼š
```python
def train_with_distortion_supervision(model, images, scores, distortion_labels):
    """
    distortion_labels: 0=clean, 1=blur, 2=noise, 3=compression, etc.
    """
    outputs = model(images, return_all=True)

    # 1. Quality loss
    quality_loss = F.mse_loss(outputs['quality_score'], scores)

    # 2. ğŸ”¥ Cluster-distortion alignment loss
    posteriors = outputs['posteriors']  # [B, K]
    distortion_onehot = F.one_hot(distortion_labels, num_classes=K)  # [B, K]

    # é¼“åŠ±posteriorsä¸distortion typeså¯¹é½
    alignment_loss = F.cross_entropy(
        torch.log(posteriors + 1e-8),
        distortion_labels
    )

    # 3. ğŸ”¥ Expert diversity loss - ç¡®ä¿ä¸åŒexpertså­¦åˆ°ä¸åŒçš„ä¸œè¥¿
    expert_outputs = outputs['expert_predictions']  # [B, K, 1]
    diversity_loss = -torch.std(expert_outputs, dim=1).mean()

    # 4. Load balancing - é¼“åŠ±ä½¿ç”¨æ‰€æœ‰experts
    avg_gates = posteriors.mean(dim=0)  # [K]
    balance_loss = torch.var(avg_gates)

    total_loss = (quality_loss +
                  0.3 * alignment_loss +
                  0.1 * diversity_loss +
                  0.2 * balance_loss)

    return total_loss
```

**ä¼˜ç‚¹**ï¼š
- âœ… å¼ºåˆ¶GMM posteriorså‘æŒ¥ä½œç”¨ï¼ˆç”¨äºé€‰æ‹©expertsï¼‰
- âœ… ä¸åŒexpertså¯ä»¥ä¸“é—¨å¤„ç†ä¸åŒdistortions
- âœ… å®ç°ç›¸å¯¹ç®€å•ï¼Œæ˜“äºè°ƒè¯•
- âœ… å¯¹åº”CVPR 2024 MoE-AGIQAçš„æ€è·¯

**é¢„æœŸæå‡**ï¼š5-10% SRCC improvement

---

### â­â­ **æ–¹æ¡ˆ2ï¼šAttention-Gated Feature Fusion**

**æ ¸å¿ƒæ€æƒ³**ï¼šç”¨attentionæœºåˆ¶è®©posteriorsè°ƒåˆ¶features

```python
class AttentionGatedCENIQA(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.backbone = build_backbone(...)
        self.gmm = DifferentiableGMM(...)

        # ğŸ”¥ Cluster-specific feature transformations
        self.cluster_projections = nn.ModuleList([
            nn.Sequential(
                nn.Linear(feature_dim, feature_dim),
                nn.LayerNorm(feature_dim),
                nn.ReLU()
            )
            for _ in range(n_clusters)
        ])

        # ğŸ”¥ Cross-attention between features and posteriors
        self.cross_attn = nn.MultiheadAttention(
            embed_dim=feature_dim,
            num_heads=8,
            batch_first=True
        )

        self.regressor = build_regressor(...)

    def forward(self, x):
        features = self.extract_features(x)  # [B, D]
        posteriors = self.gmm(features)  # [B, K]

        # æ¯ä¸ªcluster transform features
        cluster_features = []
        for k, proj in enumerate(self.cluster_projections):
            cf = proj(features)  # [B, D]
            cluster_features.append(cf)

        cluster_features = torch.stack(cluster_features, dim=1)  # [B, K, D]

        # ğŸ”¥ Posterior-weighted aggregation with attention
        # Query: original features, Key/Value: cluster features
        # Weights: posteriors
        features_expanded = features.unsqueeze(1)  # [B, 1, D]

        attended_features, attn_weights = self.cross_attn(
            features_expanded,  # query
            cluster_features,   # key
            cluster_features    # value
        )

        # Combine with posterior weights
        weighted_features = cluster_features * posteriors.unsqueeze(-1)  # [B, K, D]
        final_features = weighted_features.sum(dim=1) + attended_features.squeeze(1)

        quality_score = self.regressor(final_features)
        return quality_score.squeeze(-1)
```

**ä¼˜ç‚¹**ï¼š
- âœ… Featureså’Œposteriorsæ·±åº¦äº¤äº’
- âœ… å­¦ä¹ cluster-specific transformations
- âœ… Attentionæœºåˆ¶å¢å¼ºè¡¨è¾¾èƒ½åŠ›

**é¢„æœŸæå‡**ï¼š3-7% SRCC improvement

---

### â­â­â­ **æ–¹æ¡ˆ3ï¼šDifferentiable GMM with Learnable Priors** (æœ€åˆ›æ–°)

**æ ¸å¿ƒæ€æƒ³**ï¼šGMMå‚æ•°é€šè¿‡CNNå­¦ä¹ ï¼Œè€Œésklearnæ‹Ÿåˆ

```python
class LearnableGMM(nn.Module):
    """GMM parameters predicted by a neural network"""
    def __init__(self, feature_dim, n_clusters, hidden_dim=256):
        super().__init__()
        self.n_clusters = n_clusters
        self.feature_dim = feature_dim

        # ğŸ”¥ Network to predict GMM parameters from features
        self.param_network = nn.Sequential(
            nn.Linear(feature_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU()
        )

        # Predict means, variances, weights
        self.mean_head = nn.Linear(hidden_dim, n_clusters * feature_dim)
        self.logvar_head = nn.Linear(hidden_dim, n_clusters * feature_dim)
        self.weight_head = nn.Linear(hidden_dim, n_clusters)

    def forward(self, x):
        """
        x: [B, D] features
        Returns: posteriors [B, K]
        """
        B, D = x.shape

        # Predict GMM parameters
        h = self.param_network(x)  # [B, hidden_dim]

        means = self.mean_head(h).view(B, self.n_clusters, D)  # [B, K, D]
        log_vars = self.logvar_head(h).view(B, self.n_clusters, D)  # [B, K, D]
        log_weights = self.weight_head(h)  # [B, K]

        # Compute posteriors
        log_probs = []
        for k in range(self.n_clusters):
            diff = x.unsqueeze(1) - means[:, k:k+1, :]  # [B, 1, D]
            var = torch.exp(log_vars[:, k, :]) + 1e-6  # [B, D]

            log_prob = -0.5 * torch.sum(diff**2 / var.unsqueeze(1), dim=-1)  # [B, 1]
            log_prob -= 0.5 * torch.sum(log_vars[:, k, :], dim=-1, keepdim=True)
            log_prob += F.log_softmax(log_weights, dim=-1)[:, k:k+1]

            log_probs.append(log_prob)

        log_probs = torch.cat(log_probs, dim=1)  # [B, K]
        posteriors = F.softmax(log_probs, dim=1)

        return posteriors
```

**é…åˆContrastive Cluster Loss**ï¼š
```python
def contrastive_cluster_loss(features, posteriors, temperature=0.07):
    """
    ç¡®ä¿åŒclusterçš„featuresç›¸ä¼¼ï¼Œä¸åŒclusterçš„featuresä¸åŒ
    """
    B, K = posteriors.shape

    # Hard cluster assignments
    cluster_ids = torch.argmax(posteriors, dim=1)  # [B]

    # Compute similarity matrix
    features_norm = F.normalize(features, dim=1)
    sim_matrix = torch.matmul(features_norm, features_norm.t())  # [B, B]

    # Positive pairs: same cluster
    cluster_mask = cluster_ids.unsqueeze(0) == cluster_ids.unsqueeze(1)  # [B, B]
    cluster_mask.fill_diagonal_(False)

    # Negative pairs: different clusters
    neg_mask = ~cluster_mask
    neg_mask.fill_diagonal_(False)

    # InfoNCE loss
    sim_matrix = sim_matrix / temperature

    # For each sample, maximize similarity to same-cluster samples
    # and minimize similarity to different-cluster samples
    pos_sim = (sim_matrix * cluster_mask.float()).sum(dim=1) / (cluster_mask.sum(dim=1) + 1e-8)
    neg_sim = torch.logsumexp(sim_matrix * neg_mask.float(), dim=1)

    loss = -torch.mean(pos_sim - neg_sim)
    return loss
```

**ä¼˜ç‚¹**ï¼š
- âœ… ç«¯åˆ°ç«¯è®­ç»ƒï¼ŒGMMå‚æ•°adaptive
- âœ… å¯¹åº”Deep GMM (2024)çš„æœ€æ–°æ€è·¯
- âœ… æ›´å¼ºçš„feature-clusterè€¦åˆ

**é¢„æœŸæå‡**ï¼š7-12% SRCC improvement

---

### â­â­â­â­ **æ–¹æ¡ˆ4ï¼šDistortion-Aware Multi-Expert Architecture**

**æ ¸å¿ƒæ€æƒ³**ï¼šæ˜¾å¼å»ºæ¨¡distortion typesï¼Œç»“åˆMoEå’Œdistortion classification

```python
class DistortionAwareCENIQA(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.backbone = build_backbone(...)

        # ğŸ”¥ Distortion-aware feature extractor
        self.distortion_encoder = nn.Sequential(
            nn.Conv2d(3, 64, 3, padding=1),
            nn.ReLU(),
            nn.Conv2d(64, 128, 3, padding=1),
            nn.ReLU(),
            nn.AdaptiveAvgPool2d(1),
            nn.Flatten(),
            nn.Linear(128, 256)
        )

        # ğŸ”¥ Distortion classifier
        self.distortion_classifier = nn.Linear(256, num_distortion_types)

        # GMM clustering
        self.gmm = DifferentiableGMM(...)

        # ğŸ”¥ Distortion-specific quality experts
        self.quality_experts = nn.ModuleDict({
            'blur': build_regressor(...),
            'noise': build_regressor(...),
            'compression': build_regressor(...),
            'clean': build_regressor(...)
        })

        # Fusion network
        self.fusion = nn.Sequential(
            nn.Linear(feature_dim + 256 + n_clusters, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, 1)
        )

    def forward(self, x, return_all=False):
        # Content features
        content_features = self.backbone(x)

        # ğŸ”¥ Distortion features
        distortion_features = self.distortion_encoder(x)
        distortion_logits = self.distortion_classifier(distortion_features)
        distortion_probs = F.softmax(distortion_logits, dim=-1)

        # GMM posteriors
        posteriors = self.gmm(content_features)

        # ğŸ”¥ Distortion-specific quality prediction
        expert_outputs = []
        for distortion_type, expert in self.quality_experts.items():
            pred = expert(content_features)
            expert_outputs.append(pred)

        expert_outputs = torch.stack(expert_outputs, dim=1)  # [B, num_types, 1]

        # Weighted by distortion classification
        quality_from_experts = torch.sum(
            distortion_probs.unsqueeze(-1) * expert_outputs,
            dim=1
        )

        # Also use fusion network
        combined = torch.cat([
            content_features,
            distortion_features,
            posteriors
        ], dim=-1)
        quality_from_fusion = self.fusion(combined)

        # Final prediction
        quality_score = (quality_from_experts + quality_from_fusion) / 2

        if return_all:
            return {
                'quality_score': quality_score.squeeze(-1),
                'distortion_logits': distortion_logits,
                'posteriors': posteriors,
                'expert_outputs': expert_outputs
            }

        return quality_score.squeeze(-1)
```

**è®­ç»ƒç­–ç•¥**ï¼š
```python
def train_distortion_aware(model, images, scores, distortion_labels):
    outputs = model(images, return_all=True)

    # 1. Quality prediction loss
    quality_loss = F.mse_loss(outputs['quality_score'], scores)

    # 2. ğŸ”¥ Distortion classification loss (semi-supervised)
    if distortion_labels is not None:
        distortion_loss = F.cross_entropy(
            outputs['distortion_logits'],
            distortion_labels
        )
    else:
        # Use pseudo-labels from clustering
        pseudo_labels = torch.argmax(outputs['posteriors'], dim=1)
        distortion_loss = F.cross_entropy(
            outputs['distortion_logits'],
            pseudo_labels.detach()
        )

    # 3. ğŸ”¥ Consistency loss - distortion classification should match GMM clustering
    distortion_probs = F.softmax(outputs['distortion_logits'], dim=-1)
    consistency_loss = F.kl_div(
        torch.log(outputs['posteriors'] + 1e-8),
        distortion_probs,
        reduction='batchmean'
    )

    total_loss = quality_loss + 0.3 * distortion_loss + 0.2 * consistency_loss

    return total_loss
```

**ä¼˜ç‚¹**ï¼š
- âœ… æ˜¾å¼å»ºæ¨¡distortion types
- âœ… Multi-level feature fusion
- âœ… å¯¹åº”CDINet (2024)çš„distortion-awareæ€è·¯

**é¢„æœŸæå‡**ï¼š10-15% SRCC improvement

---

### â­â­â­â­â­ **æ–¹æ¡ˆ5ï¼šå®Œæ•´çš„Self-Supervised GMM-IQA Pipeline**

**æ ¸å¿ƒæ€æƒ³**ï¼šç»“åˆæ‰€æœ‰æœ€ä½³å®è·µï¼Œæ„å»ºå®Œæ•´çš„pipeline

è¿™ä¸ªæ–¹æ¡ˆç»“åˆï¼š
1. Contrastive learning for distortion-aware features
2. Learnable GMM with differentiable EM
3. Mixture of Experts with gating
4. Self-supervised cluster-distortion alignment
5. Monotonic constraints on regressors

ç”±äºç¯‡å¹…é™åˆ¶ï¼Œè¿™ä¸ªæ–¹æ¡ˆçš„å®Œæ•´å®ç°éœ€è¦å•ç‹¬çš„æ–‡ä»¶ã€‚

---

## ğŸ¯ **æ¨èå®æ–½é¡ºåº**

### Week 1: **æ–¹æ¡ˆ1 - MoE Regressors**
- æœ€å®¹æ˜“å®ç°
- ç«‹å³å¯è§æ•ˆæœ
- éªŒè¯GMMæ˜¯å¦çœŸçš„æœ‰ç”¨

### Week 2: **æ–¹æ¡ˆ3 - Learnable GMM**
- æ›¿æ¢sklearn GMM
- åŠ å…¥contrastive cluster loss
- ç«¯åˆ°ç«¯è®­ç»ƒ

### Week 3: **æ–¹æ¡ˆ4 - Distortion-Aware**
- æ·»åŠ distortion classification branch
- å®ç°distortion-specific experts
- æå‡interpretability

### Week 4: **é›†æˆå’Œä¼˜åŒ–**
- ç»“åˆæœ€ä½³ç»„ä»¶
- è¶…å‚æ•°è°ƒä¼˜
- å‡†å¤‡è®ºæ–‡å®éªŒ

---

## ğŸ“Š **å…³é”®æ”¹è¿›ç‚¹æ€»ç»“**

| æ”¹è¿›ç‚¹ | å½“å‰å®ç° | æ–°æ–¹æ¡ˆ | é¢„æœŸæå‡ |
|--------|---------|--------|---------|
| **Posterioråˆ©ç”¨** | Simple concat | MoE gating | â­â­â­â­â­ |
| **GMMè®­ç»ƒ** | Sklearn offline | Learnable/differentiable | â­â­â­â­ |
| **Clusterè¯­ä¹‰** | Unsupervised | Distortion-aligned | â­â­â­â­â­ |
| **Featureäº¤äº’** | None | Attention/cross-attn | â­â­â­ |
| **Expert diversity** | Single regressor | Multiple experts | â­â­â­â­ |
| **Losses** | Simple cluster loss | Contrastive + consistency | â­â­â­â­ |

---

## ğŸ”§ **å¿«é€Ÿå¯åŠ¨ï¼šå®ç°æ–¹æ¡ˆ1**

1. **ä¿®æ”¹`model.py`**ï¼š
   ```bash
   cp model.py model_backup.py
   # å®ç°MoECENIQAç±»
   ```

2. **æ›´æ–°`train_high_res.py`**ï¼š
   ```bash
   # æ·»åŠ expert diversity loss
   # æ·»åŠ distortion labelsï¼ˆå¯ç”¨synthetic distortionsè‡ªåŠ¨ç”Ÿæˆï¼‰
   ```

3. **è®­ç»ƒå¯¹æ¯”**ï¼š
   ```bash
   # Baseline
   python train_simple_high_res.py --experiment_name baseline_v2

   # MoE version
   python train_high_res.py --use_moe --experiment_name moe_v1 --n_experts 5
   ```

---

## ğŸ“– **å‚è€ƒæ–‡çŒ®**

1. **MoE-AGIQA** (CVPR 2024): Mixture-of-Experts for AI-Generated Image Quality Assessment
2. **Deep GMM** (April 2024): Deep Gaussian mixture model for unsupervised image segmentation
3. **CDINet** (IEEE TMM 2024): Content Distortion Interaction Network for BIQA
4. **Attention Clustering** (Feb 2024): Deep clustering using 3D attention convolutional autoencoder
5. **Differentiable Clustering** (July 2024): Differentiable self-supervised clustering with intrinsic interpretability

---

## ğŸ’¬ **éœ€è¦æˆ‘å¸®å¿™å®ç°å—ï¼Ÿ**

æˆ‘å¯ä»¥å¸®ä½ ï¼š
1. âœ… å®Œæ•´å®ç°æ–¹æ¡ˆ1çš„ä»£ç ï¼ˆMoE Regressorsï¼‰
2. âœ… ä¿®æ”¹è®­ç»ƒè„šæœ¬æ”¯æŒæ–°losses
3. âœ… åˆ›å»ºå¯¹æ¯”å®éªŒé…ç½®
4. âœ… å®ç°å¯è§†åŒ–å·¥å…·åˆ†æclusterså’Œexperts

é€‰æ‹©ä½ æƒ³å…ˆå®ç°å“ªä¸ªæ–¹æ¡ˆï¼Œæˆ‘ä¼šæä¾›å®Œæ•´çš„ä»£ç ï¼
