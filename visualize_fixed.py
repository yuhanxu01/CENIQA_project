"""
é‡å†™çš„å¯è§†åŒ–è„šæœ¬ - ä¿®å¤æ‰€æœ‰é—®é¢˜
æ”¯æŒ DistortedImageDatasetï¼Œæ­£ç¡®çš„èšç±»åˆ†å¸ƒï¼Œæ¸…æ™°çš„å›¾åƒæ˜¾ç¤º

Usage:
    python visualize_fixed.py --experiment experiments/resnet18_large_10k --checkpoint best_model.pth
"""
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import numpy as np
import json
import argparse
from pathlib import Path
from tqdm import tqdm
import matplotlib.pyplot as plt
import matplotlib.gridspec as gridspec
from matplotlib.patches import Rectangle
import seaborn as sns
from scipy.stats import spearmanr, pearsonr

from train_gpu import SimpleCNNGMMMLPModel
from distorted_dataset import DistortedImageDataset


def denormalize_image(tensor):
    """
    åå½’ä¸€åŒ–å›¾åƒç”¨äºæ˜¾ç¤º
    ImageNet normalization: mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]
    """
    mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)
    std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)

    if tensor.device != 'cpu':
        tensor = tensor.cpu()

    img = tensor * std + mean
    img = torch.clamp(img, 0, 1)
    img = img.permute(1, 2, 0).numpy()
    return img


def load_model_from_checkpoint(checkpoint_path, device):
    """ä»æ£€æŸ¥ç‚¹åŠ è½½æ¨¡å‹"""
    checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)
    state_dict = checkpoint['model_state_dict']

    # æ¨æ–­æ¨¡å‹é…ç½®
    feature_dim = state_dict['backbone.proj.weight'].shape[0]
    n_clusters = state_dict['gmm.means'].shape[0]

    if 'regressor.fc1.weight' in state_dict:
        hidden_dim = state_dict['regressor.fc1.weight'].shape[0]
    else:
        hidden_dim = 512

    print(f"Model config inferred:")
    print(f"  - feature_dim: {feature_dim}")
    print(f"  - n_clusters: {n_clusters}")
    print(f"  - hidden_dim: {hidden_dim}")

    # åˆ›å»ºæ¨¡å‹
    model = SimpleCNNGMMMLPModel(
        backbone_name='resnet18',
        feature_dim=feature_dim,
        n_clusters=n_clusters,
        hidden_dim=hidden_dim,
        dropout=0.3,
        freeze_backbone=False
    )

    model.load_state_dict(state_dict)
    model = model.to(device)
    model.eval()

    return model, n_clusters


def run_inference(model, dataloader, device):
    """è¿è¡Œæ¨æ–­å¹¶æ”¶é›†ç»“æœ"""
    model.eval()

    all_predictions = []
    all_targets = []
    all_posteriors = []
    all_images = []
    all_distortion_types = []

    print("\nRunning inference...")
    with torch.no_grad():
        for batch in tqdm(dataloader, desc='Inference'):
            if len(batch) == 3:
                images, scores, distortion_types = batch
            else:
                images, scores = batch
                distortion_types = None

            images = images.to(device)

            # å‰å‘ä¼ æ’­
            outputs = model(images, return_all=True)

            # æ”¶é›†ç»“æœ
            all_predictions.append(outputs['quality_score'].cpu().numpy())
            all_targets.append(scores.numpy())
            all_posteriors.append(outputs['posteriors'].cpu().numpy())
            all_images.append(images.cpu())

            if distortion_types is not None:
                all_distortion_types.extend(distortion_types)

    # åˆå¹¶æ‰€æœ‰æ‰¹æ¬¡
    predictions = np.concatenate(all_predictions)
    targets = np.concatenate(all_targets)
    posteriors = np.concatenate(all_posteriors)
    images = torch.cat(all_images, dim=0)

    # è·å–èšç±»åˆ†é…
    cluster_ids = np.argmax(posteriors, axis=1)

    return {
        'predictions': predictions,
        'targets': targets,
        'posteriors': posteriors,
        'cluster_ids': cluster_ids,
        'images': images,
        'distortion_types': all_distortion_types if all_distortion_types else None
    }


def select_diverse_samples(cluster_ids, predictions, targets, n_samples=25):
    """
    é€‰æ‹©å¤šæ ·åŒ–çš„æ ·æœ¬ï¼Œç¡®ä¿è¦†ç›–æ‰€æœ‰èšç±»
    """
    n_clusters = len(np.unique(cluster_ids))
    samples_per_cluster = max(1, n_samples // n_clusters)

    selected_indices = []

    for cluster_id in range(n_clusters):
        cluster_mask = cluster_ids == cluster_id
        cluster_indices = np.where(cluster_mask)[0]

        if len(cluster_indices) == 0:
            print(f"Warning: Cluster {cluster_id} has no samples")
            continue

        # ä»æ¯ä¸ªèšç±»ä¸­é€‰æ‹©æ ·æœ¬ï¼Œå°½é‡è¦†ç›–ä¸åŒçš„è´¨é‡èŒƒå›´
        n_select = min(samples_per_cluster, len(cluster_indices))

        # æŒ‰é¢„æµ‹åˆ†æ•°æ’åºï¼Œé€‰æ‹©å‡åŒ€åˆ†å¸ƒçš„æ ·æœ¬
        cluster_preds = predictions[cluster_indices]
        sorted_idx = cluster_indices[np.argsort(cluster_preds)]

        # å‡åŒ€é‡‡æ ·
        step = len(sorted_idx) // n_select if n_select > 0 else 1
        selected = sorted_idx[::step][:n_select]

        selected_indices.extend(selected.tolist())

    # å¦‚æœæ ·æœ¬æ•°ä¸å¤Ÿï¼Œéšæœºè¡¥å……
    while len(selected_indices) < n_samples and len(selected_indices) < len(predictions):
        remaining = list(set(range(len(predictions))) - set(selected_indices))
        if remaining:
            selected_indices.append(np.random.choice(remaining))
        else:
            break

    return np.array(selected_indices[:n_samples])


def plot_image_grid(images, predictions, targets, cluster_ids, posteriors,
                    distortion_types=None, save_path=None):
    """
    ç»˜åˆ¶å›¾åƒç½‘æ ¼ï¼Œæ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
    """
    n_images = len(images)
    n_cols = 5
    n_rows = (n_images + n_cols - 1) // n_cols

    fig, axes = plt.subplots(n_rows, n_cols, figsize=(n_cols*3, n_rows*3.5))
    if n_rows == 1:
        axes = axes.reshape(1, -1)

    for idx in range(n_rows * n_cols):
        row = idx // n_cols
        col = idx % n_cols
        ax = axes[row, col]

        if idx < n_images:
            # æ˜¾ç¤ºå›¾åƒ
            img = denormalize_image(images[idx])
            ax.imshow(img)
            ax.axis('off')

            # è·å–ä¿¡æ¯
            pred = predictions[idx]
            target = targets[idx]
            cluster = cluster_ids[idx]
            confidence = posteriors[idx][cluster]
            error = abs(pred - target)

            # é¢œè‰²ç¼–ç ï¼šæ ¹æ®è¯¯å·®
            if error < 0.1:
                color = 'green'
            elif error < 0.2:
                color = 'orange'
            else:
                color = 'red'

            # æ ‡é¢˜
            title_parts = [
                f'Pred: {pred:.3f}',
                f'GT: {target:.3f}',
                f'Cluster: {cluster}',
                f'Conf: {confidence:.2f}',
                f'Err: {error:.3f}'
            ]

            if distortion_types and idx < len(distortion_types):
                dist_type = distortion_types[idx]
                if dist_type != 'pristine':
                    title_parts.insert(0, f'[{dist_type}]')

            title = '\n'.join(title_parts)
            ax.set_title(title, fontsize=8, color=color, weight='bold', pad=5)

            # è¾¹æ¡†
            for spine in ax.spines.values():
                spine.set_edgecolor(color)
                spine.set_linewidth(3)
                spine.set_visible(True)
        else:
            ax.axis('off')

    plt.suptitle(
        'Image Quality Assessment - Diverse Samples from All Clusters\n'
        'Green: Error < 0.1 | Orange: 0.1 â‰¤ Error < 0.2 | Red: Error â‰¥ 0.2',
        fontsize=14, weight='bold', y=1.0
    )
    plt.tight_layout()

    if save_path:
        plt.savefig(save_path, dpi=150, bbox_inches='tight')
        print(f"âœ… Saved: {save_path}")

    plt.close()


def plot_cluster_examples(images, predictions, targets, cluster_ids, posteriors,
                          n_clusters, n_per_cluster=6, save_path=None):
    """
    æ¯ä¸ªèšç±»æ˜¾ç¤ºä»£è¡¨æ€§æ ·æœ¬
    """
    fig, axes = plt.subplots(n_clusters, n_per_cluster,
                            figsize=(n_per_cluster*2.5, n_clusters*2.5))

    if n_clusters == 1:
        axes = axes.reshape(1, -1)

    for cluster_id in range(n_clusters):
        cluster_mask = cluster_ids == cluster_id
        cluster_indices = np.where(cluster_mask)[0]

        cluster_count = len(cluster_indices)

        if cluster_count == 0:
            # ç©ºèšç±»
            for i in range(n_per_cluster):
                ax = axes[cluster_id, i] if n_clusters > 1 else axes[i]
                ax.text(0.5, 0.5, 'No\nSamples', ha='center', va='center', fontsize=12)
                ax.set_xlim(0, 1)
                ax.set_ylim(0, 1)
                ax.axis('off')
            continue

        # é€‰æ‹©ä»£è¡¨æ€§æ ·æœ¬
        n_select = min(n_per_cluster, cluster_count)

        # æŒ‰è´¨é‡åˆ†æ•°æ’åºåå‡åŒ€é€‰æ‹©
        cluster_preds = predictions[cluster_indices]
        sorted_idx = cluster_indices[np.argsort(cluster_preds)]

        if cluster_count >= n_per_cluster:
            step = cluster_count // n_per_cluster
            selected = sorted_idx[::step][:n_per_cluster]
        else:
            selected = sorted_idx

        for i in range(n_per_cluster):
            ax = axes[cluster_id, i] if n_clusters > 1 else axes[i]

            if i < len(selected):
                idx = selected[i]

                # æ˜¾ç¤ºå›¾åƒ
                img = denormalize_image(images[idx])
                ax.imshow(img)
                ax.axis('off')

                # ä¿¡æ¯
                pred = predictions[idx]
                target = targets[idx]
                conf = posteriors[idx][cluster_id]
                error = abs(pred - target)

                color = 'green' if error < 0.1 else 'orange' if error < 0.2 else 'red'

                title = f'P:{pred:.2f} T:{target:.2f}\nConf:{conf:.2f}'
                ax.set_title(title, fontsize=8, color=color)

                for spine in ax.spines.values():
                    spine.set_edgecolor(color)
                    spine.set_linewidth(2)
                    spine.set_visible(True)
            else:
                ax.axis('off')

        # èšç±»æ ‡ç­¾ï¼ˆåœ¨ç¬¬ä¸€ä¸ªå­å›¾çš„å·¦ä¾§ï¼‰
        ax = axes[cluster_id, 0] if n_clusters > 1 else axes[0]
        cluster_avg_quality = targets[cluster_mask].mean()
        cluster_avg_pred = predictions[cluster_mask].mean()
        ax.text(-0.15, 0.5,
                f'Cluster {cluster_id}\n({cluster_count} samples)\nAvg Q: {cluster_avg_quality:.2f}\nAvg P: {cluster_avg_pred:.2f}',
                transform=ax.transAxes, ha='right', va='center', fontsize=9,
                bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.7))

    plt.suptitle(f'Representative Samples from Each Cluster',
                fontsize=14, weight='bold')
    plt.tight_layout()

    if save_path:
        plt.savefig(save_path, dpi=150, bbox_inches='tight')
        print(f"âœ… Saved: {save_path}")

    plt.close()


def plot_performance_metrics(predictions, targets, cluster_ids, save_path=None):
    """
    æ€§èƒ½æŒ‡æ ‡å¯è§†åŒ–
    """
    fig, axes = plt.subplots(2, 2, figsize=(12, 10))

    # è®¡ç®—æ€»ä½“æŒ‡æ ‡
    srcc, _ = spearmanr(predictions, targets)
    plcc, _ = pearsonr(predictions, targets)
    rmse = np.sqrt(np.mean((predictions - targets) ** 2))
    mae = np.mean(np.abs(predictions - targets))

    # 1. æ•£ç‚¹å›¾ï¼šé¢„æµ‹ vs çœŸå®
    ax = axes[0, 0]
    ax.scatter(targets, predictions, alpha=0.5, s=30, c='steelblue', edgecolors='k', linewidth=0.5)

    # å¯¹è§’çº¿
    min_val = min(targets.min(), predictions.min())
    max_val = max(targets.max(), predictions.max())
    ax.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='Perfect')

    ax.set_xlabel('Ground Truth Score', fontsize=12, weight='bold')
    ax.set_ylabel('Predicted Score', fontsize=12, weight='bold')
    ax.set_title('Predictions vs Ground Truth', fontsize=13, weight='bold')
    ax.legend(fontsize=10)
    ax.grid(True, alpha=0.3)

    # æ·»åŠ æŒ‡æ ‡æ–‡æœ¬
    metrics_text = f'SRCC: {srcc:.4f}\nPLCC: {plcc:.4f}\nRMSE: {rmse:.4f}\nMAE: {mae:.4f}'
    ax.text(0.05, 0.95, metrics_text, transform=ax.transAxes,
            fontsize=11, verticalalignment='top',
            bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))

    # 2. è¯¯å·®åˆ†å¸ƒ
    ax = axes[0, 1]
    errors = predictions - targets
    ax.hist(errors, bins=50, color='steelblue', alpha=0.7, edgecolor='black')
    ax.axvline(x=0, color='red', linestyle='--', linewidth=2, label='Zero Error')
    ax.axvline(x=errors.mean(), color='green', linestyle='--', linewidth=2,
               label=f'Mean: {errors.mean():.4f}')
    ax.set_xlabel('Prediction Error', fontsize=12, weight='bold')
    ax.set_ylabel('Frequency', fontsize=12, weight='bold')
    ax.set_title('Error Distribution', fontsize=13, weight='bold')
    ax.legend(fontsize=10)
    ax.grid(True, alpha=0.3)

    # 3. æ¯ä¸ªèšç±»çš„æ€§èƒ½
    ax = axes[1, 0]
    n_clusters = len(np.unique(cluster_ids))
    cluster_metrics = []

    for i in range(n_clusters):
        mask = cluster_ids == i
        if mask.sum() > 0:
            cluster_preds = predictions[mask]
            cluster_targets = targets[mask]
            cluster_srcc, _ = spearmanr(cluster_preds, cluster_targets)
            cluster_metrics.append((i, mask.sum(), cluster_srcc))

    cluster_ids_plot = [x[0] for x in cluster_metrics]
    cluster_counts = [x[1] for x in cluster_metrics]
    cluster_srccs = [x[2] for x in cluster_metrics]

    x_pos = np.arange(len(cluster_ids_plot))
    bars = ax.bar(x_pos, cluster_srccs, color='steelblue', alpha=0.7, edgecolor='black')

    # é¢œè‰²ç¼–ç 
    for i, (bar, srcc_val) in enumerate(zip(bars, cluster_srccs)):
        if srcc_val >= 0.7:
            bar.set_color('green')
        elif srcc_val >= 0.5:
            bar.set_color('orange')
        else:
            bar.set_color('red')

    ax.set_xlabel('Cluster ID', fontsize=12, weight='bold')
    ax.set_ylabel('SRCC', fontsize=12, weight='bold')
    ax.set_title('Per-Cluster Performance', fontsize=13, weight='bold')
    ax.set_xticks(x_pos)
    ax.set_xticklabels([f'{cid}\n({cnt})' for cid, cnt in zip(cluster_ids_plot, cluster_counts)])
    ax.axhline(y=srcc, color='red', linestyle='--', linewidth=2, label=f'Overall: {srcc:.3f}')
    ax.legend(fontsize=10)
    ax.grid(True, alpha=0.3, axis='y')

    # 4. èšç±»åˆ†å¸ƒ
    ax = axes[1, 1]
    unique_clusters, counts = np.unique(cluster_ids, return_counts=True)
    colors = plt.cm.Set3(np.linspace(0, 1, len(unique_clusters)))

    wedges, texts, autotexts = ax.pie(counts, labels=[f'C{c}' for c in unique_clusters],
                                        autopct='%1.1f%%', colors=colors,
                                        startangle=90, textprops={'fontsize': 10, 'weight': 'bold'})

    # æ·»åŠ å›¾ä¾‹
    legend_labels = [f'Cluster {c}: {cnt} samples' for c, cnt in zip(unique_clusters, counts)]
    ax.legend(legend_labels, loc='center left', bbox_to_anchor=(1, 0, 0.5, 1), fontsize=9)
    ax.set_title('Cluster Distribution', fontsize=13, weight='bold')

    plt.suptitle(f'Performance Metrics Dashboard\n'
                 f'Overall - SRCC: {srcc:.4f} | PLCC: {plcc:.4f} | RMSE: {rmse:.4f}',
                 fontsize=15, weight='bold')
    plt.tight_layout()

    if save_path:
        plt.savefig(save_path, dpi=150, bbox_inches='tight')
        print(f"âœ… Saved: {save_path}")

    plt.close()


def main():
    parser = argparse.ArgumentParser(description='Fixed visualization for distorted images')
    parser.add_argument('--experiment', type=str, required=True,
                       help='Experiment directory')
    parser.add_argument('--checkpoint', type=str, default='best_model.pth',
                       help='Checkpoint filename')
    parser.add_argument('--test_samples', type=int, default=500,
                       help='Number of test samples')
    parser.add_argument('--num_display', type=int, default=25,
                       help='Number of images to display in grid')
    parser.add_argument('--batch_size', type=int, default=64,
                       help='Batch size for inference')
    args = parser.parse_args()

    exp_dir = Path(args.experiment)
    checkpoint_path = exp_dir / args.checkpoint

    if not checkpoint_path.exists():
        print(f"âŒ Error: Checkpoint not found: {checkpoint_path}")
        return

    print("="*80)
    print("ğŸ¨ Fixed Visualization for Distorted Image Quality Assessment")
    print("="*80)
    print(f"Experiment: {exp_dir.name}")
    print(f"Checkpoint: {args.checkpoint}")
    print("="*80)

    # è®¾ç½®è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"\nDevice: {device}")

    # åŠ è½½æ¨¡å‹
    print("\nğŸ“¦ Loading model...")
    model, n_clusters = load_model_from_checkpoint(checkpoint_path, device)
    print("âœ… Model loaded successfully")

    # åŠ è½½æµ‹è¯•æ•°æ®é›†ï¼ˆä½¿ç”¨æ­£ç¡®çš„DistortedImageDatasetï¼ï¼‰
    print("\nğŸ“Š Loading test dataset...")
    test_dataset = DistortedImageDataset(
        split='test',
        max_samples=args.test_samples,
        distortions_per_image=5,
        include_pristine=True
    )

    test_loader = DataLoader(
        test_dataset,
        batch_size=args.batch_size,
        shuffle=False,
        num_workers=2,
        pin_memory=True
    )

    # è¿è¡Œæ¨æ–­
    results = run_inference(model, test_loader, device)

    # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
    print("\n" + "="*80)
    print("ğŸ“ˆ Inference Results")
    print("="*80)
    print(f"Total samples: {len(results['predictions'])}")
    print(f"Number of clusters: {n_clusters}")

    srcc, _ = spearmanr(results['predictions'], results['targets'])
    plcc, _ = pearsonr(results['predictions'], results['targets'])
    rmse = np.sqrt(np.mean((results['predictions'] - results['targets']) ** 2))

    print(f"\nOverall Performance:")
    print(f"  SRCC: {srcc:.4f}")
    print(f"  PLCC: {plcc:.4f}")
    print(f"  RMSE: {rmse:.4f}")

    print(f"\nCluster Distribution:")
    unique_clusters, counts = np.unique(results['cluster_ids'], return_counts=True)
    for cluster_id, count in zip(unique_clusters, counts):
        mask = results['cluster_ids'] == cluster_id
        avg_quality = results['targets'][mask].mean()
        print(f"  Cluster {cluster_id}: {count:4d} samples (avg quality: {avg_quality:.3f})")

    # åˆ›å»ºè¾“å‡ºç›®å½•
    viz_dir = exp_dir / 'visualizations_fixed'
    viz_dir.mkdir(exist_ok=True)

    print("\n" + "="*80)
    print("ğŸ¨ Generating Visualizations")
    print("="*80)

    # 1. æ€§èƒ½æŒ‡æ ‡ä»ªè¡¨æ¿
    print("\n1. Generating performance metrics dashboard...")
    plot_performance_metrics(
        results['predictions'],
        results['targets'],
        results['cluster_ids'],
        save_path=viz_dir / 'performance_metrics.png'
    )

    # 2. é€‰æ‹©å¤šæ ·åŒ–æ ·æœ¬å¹¶ç»˜åˆ¶
    print(f"\n2. Selecting {args.num_display} diverse samples from all clusters...")
    selected_indices = select_diverse_samples(
        results['cluster_ids'],
        results['predictions'],
        results['targets'],
        n_samples=args.num_display
    )

    print(f"   Selected samples distribution:")
    for cluster_id in range(n_clusters):
        count = np.sum(results['cluster_ids'][selected_indices] == cluster_id)
        print(f"     Cluster {cluster_id}: {count} samples")

    print("\n3. Generating image grid...")
    plot_image_grid(
        results['images'][selected_indices],
        results['predictions'][selected_indices],
        results['targets'][selected_indices],
        results['cluster_ids'][selected_indices],
        results['posteriors'][selected_indices],
        distortion_types=[results['distortion_types'][i] for i in selected_indices] if results['distortion_types'] else None,
        save_path=viz_dir / 'image_grid_diverse.png'
    )

    # 3. æ¯ä¸ªèšç±»çš„ä»£è¡¨æ€§æ ·æœ¬
    print("\n4. Generating cluster examples...")
    plot_cluster_examples(
        results['images'],
        results['predictions'],
        results['targets'],
        results['cluster_ids'],
        results['posteriors'],
        n_clusters=n_clusters,
        n_per_cluster=6,
        save_path=viz_dir / 'cluster_examples_detailed.png'
    )

    print("\n" + "="*80)
    print("âœ… Visualization Complete!")
    print("="*80)
    print(f"\nAll visualizations saved to: {viz_dir}")
    print("\nGenerated files:")
    print(f"  1. performance_metrics.png - Overall performance dashboard")
    print(f"  2. image_grid_diverse.png - {args.num_display} diverse samples from all clusters")
    print(f"  3. cluster_examples_detailed.png - Representative samples from each cluster")
    print("\n" + "="*80)


if __name__ == '__main__':
    main()
